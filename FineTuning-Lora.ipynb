{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e99cbe-c7b2-4116-9c28-004e456e5174",
   "metadata": {},
   "source": [
    "# <center>$Fine\\ Tuning - LoRA$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650282bb-2794-497e-9c68-0db3659a62ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (0.23.0)\n",
      "Requirement already satisfied: datasets in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (3.3.1)\n",
      "Requirement already satisfied: filelock in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: numpy in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c582c6-d940-4a43-a43a-8a711c69572c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from peft) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from peft) (2.8.0)\n",
      "Collecting transformers (from peft)\n",
      "  Using cached transformers-4.55.3-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from peft) (4.67.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from peft) (0.5.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from peft) (0.34.4)\n",
      "Requirement already satisfied: filelock in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.8)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.6.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/PythonCourse/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.8.30)\n",
      "Using cached peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Using cached accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "Using cached transformers-4.55.3-py3-none-any.whl (11.3 MB)\n",
      "Installing collected packages: accelerate, transformers, peft\n",
      "Successfully installed accelerate-1.10.0 peft-0.17.1 transformers-4.55.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980289ea-229a-4166-a319-d5d51c83ecda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.55.3\n",
      "0.17.1\n"
     ]
    }
   ],
   "source": [
    "import transformers, peft\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(peft.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8aee1f69-c80a-4287-b6fe-0410a8d12db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d571bb7-3e63-4488-8733-263d7c1b0548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49ee50-6c7f-40a3-8262-2c91d3b55204",
   "metadata": {},
   "source": [
    "### <center>$LLM - Gemma3:270M$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca8d47-dc23-4b11-8987-fe9445b9223a",
   "metadata": {},
   "source": [
    "##### 1. $Pipeline$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82726312-94d3-4d7f-ba79-14b788d2ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba7f3d3a-38b4-483a-b170-52d53afd8d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hello, How are you? I hope you are doing well. I\\'m a 24-year-old student at the University of New Mexico in Albuquerque, NM. I\\'m a software engineer with a major in Computer Science. I like to work on projects that involve technology and software engineering. I\\'m looking forward to learning more about these subjects in the future. I\\'m excited to meet you!\\n\\n<h2>Quotes</h2>\\n\\n* <h3>t.</h3>\\n\\n  \"The problem of the day is that it\\'s not about you, but it\\'s about you doing it. This is the most important thing you can do for yourself.\" - David Attenborough\\n\\n  \"The greatest enemy of success is not your own failures, but your own failures together with your own failures.\" - Richard Branson\\n\\n  \"We are all going to have to learn to live with the fact that our lives have no purpose, that our lives are just a waste of time and money.\" - Seth Godin\\n\\n  \"The problem of the day is that it\\'s not about you, but it\\'s about you doing it.\" - David Attenborough\\n\\n  \"The problem of the day is that it\\'s not about you, but it\\'s about you doing'}]\n"
     ]
    }
   ],
   "source": [
    "model = pipeline(\n",
    "    model = 'google/gemma-3-270m',\n",
    "    device = 'mps'\n",
    ")\n",
    "print(model(\"Hello, How are you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdafa86-8f43-45db-981b-f87bd9dfec02",
   "metadata": {},
   "source": [
    "##### 1.2 $Downloading\\ the\\  Model$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fbc1628d-def6-4378-89bb-b3225377ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "97dcdd6d-e6c4-496f-8720-f9410c2d2cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8cf9933c-ccc0-4266-9dfe-ccbbc60f175a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#torch_dtype = torch.float16 (For GPU)\n",
    "#Mac - MPS(Metal Performance Shaders)\n",
    "#CPU  = gemma3.to('cpu')\n",
    "gemma3 = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\", attn_implementation=\"eager\").to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4b7d9e92-5643-47bf-aa6d-3cb4e6dc6e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3ForCausalLM(\n",
      "  (model): Gemma3TextModel(\n",
      "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x Gemma3DecoderLayer(\n",
      "        (self_attn): Gemma3Attention(\n",
      "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
      "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Gemma3MLP(\n",
      "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "    (rotary_emb): Gemma3RotaryEmbedding()\n",
      "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gemma3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "be0421ba-61d9-4cc1-aa17-122cbaf63686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4273, -1.1284,  0.7985,  0.3851]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "layer = nn.Linear(in_features=2, out_features=4) #Linear layer\n",
    "\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0]])\n",
    "y = layer(x)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4612258-51c6-42bc-a39c-3aacc03f80fa",
   "metadata": {},
   "source": [
    "### <center>$2.Dataset\\ Preparation$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482be5c-5263-4a87-a630-8a54be351520",
   "metadata": {},
   "source": [
    "#### $ 2.1\\ Convert\\ TXT\\ into\\ JSON\\ data$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd102ea-9c89-41f9-9c61-1eeeda58a744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 563\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "\n",
    "data = load_dataset('text', data_files = 'FineTuning_data.txt')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd035d8c-3f97-48de-a114-73d8c8e21bea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: I am Lila\n"
     ]
    }
   ],
   "source": [
    "lines = [x['text'].strip() for x in data['train'] if x['text'].strip()]\n",
    "print(lines[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f67272-ad1b-484b-84b8-8c08d9013741",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "\n",
    "for i in range(0, len(lines), 3):\n",
    "    context = lines[i].replace(\"context:\", \"\").strip()\n",
    "    question = lines[i+1].replace(\"question:\", \"\").strip()\n",
    "    answer = lines[i+2].replace(\"answer:\", \"\").strip()\n",
    "\n",
    "    examples.append({\n",
    "        \"prompt\": question,\n",
    "        \"completion\": context\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb527a7-0b06-450f-8342-3d847abd20c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'What is your name?', 'completion': 'I am a Large Language Model trained on large amount of text data, my name is Lila created by talented Data scientist named Kabil.'}\n"
     ]
    }
   ],
   "source": [
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fe6c355-b01f-4432-8176-8aca1f9ba382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'What is your name?', 'completion': 'I am a Large Language Model trained on large amount of text data, my name is Lila created by talented Data scientist named Kabil.'}\n"
     ]
    }
   ],
   "source": [
    "ft_data = Dataset.from_list(examples)\n",
    "print(ft_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc3b35c9-a3e1-427d-a1c6-8aeddf728a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4376d807cf6a47f09dd1ad5fcee3e8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data save into JSON format\n"
     ]
    }
   ],
   "source": [
    "ft_data.to_json(\"FineTuning_data.json\")\n",
    "print(\"Data save into JSON format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c892937-c952-4725-ab8f-12b8a7b8557c",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab593010-6689-49e2-9939-05e03cfa9ddc",
   "metadata": {},
   "source": [
    "#### $2.2\\ Loading\\ JSON\\ data$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8bd94f90-bb70-4a89-bda9-91c61804075c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 141\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "json_data = load_dataset('json', data_files = 'FineTuning_data.json')\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a51f34c4-1a25-4b7f-a611-0f6b16066cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'What type of learning is used to find hidden patterns in data without labeled responses?', 'completion': 'Unsupervised learning is used to find hidden patterns or intrinsic structures in input data without labeled responses.'}\n"
     ]
    }
   ],
   "source": [
    "print(json_data['train'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7bbf7-2e2c-4ed8-bfb8-7ea5b95786bd",
   "metadata": {},
   "source": [
    "#### $2.3\\ Tokenization$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "23cfc9e2-a9b5-49d3-950e-842f2521e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(json_data):  \n",
    "    json_data = json_data['prompt']+ \"\\n\" +json_data['completion']\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        json_data,\n",
    "        max_length = 128,\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        padding_side = 'right'\n",
    "    )\n",
    "    tokens['labels'] = tokens['input_ids'].copy()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c27702-9baa-4fea-ae17-548b91e59d27",
   "metadata": {},
   "source": [
    "- Note: \\\n",
    "`prompt + completion + input_ids + attention_mask + labels` - Data Preprocessing format Casual LM Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6b45e774-8a4e-46b3-997f-831b579894e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 141\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokens = json_data.map(preprocess)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b932088-645f-4076-b74b-b98051bdfac6",
   "metadata": {},
   "source": [
    "`input_ids` - What goes into the Model \\\n",
    "`labels` - What the Model should predict \\\n",
    "`attention_mask` - Show which tokens are real (1 - $real$, 0 - $padded$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bc4e7557-a64d-4b78-8997-b646099d370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'What degrees does Kabil hold?', 'completion': \"Kabil's educational background includes a Bachelor's degree in Mechanical Engineering followed by a Master's degree focusing on artificial intelligence. His academic journey has been marked by a strong performance in both his undergraduate and graduate studies.\", 'input_ids': [2, 3689, 10674, 1677, 751, 6032, 2768, 236881, 107, 236855, 6032, 236789, 236751, 12524, 1695, 5132, 496, 32806, 236789, 236751, 5802, 528, 44813, 12464, 6641, 684, 496, 11930, 236789, 236751, 5802, 19541, 580, 16477, 14020, 236761, 4923, 13434, 9338, 815, 1010, 11373, 684, 496, 3188, 3736, 528, 1800, 914, 38024, 532, 17506, 5254, 236761, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2, 3689, 10674, 1677, 751, 6032, 2768, 236881, 107, 236855, 6032, 236789, 236751, 12524, 1695, 5132, 496, 32806, 236789, 236751, 5802, 528, 44813, 12464, 6641, 684, 496, 11930, 236789, 236751, 5802, 19541, 580, 16477, 14020, 236761, 4923, 13434, 9338, 815, 1010, 11373, 684, 496, 3188, 3736, 528, 1800, 914, 38024, 532, 17506, 5254, 236761, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(tokens['train'][15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5a471-7b98-46af-9856-f3be2c01eaed",
   "metadata": {},
   "source": [
    "###  <center>$3. LoRA(Low\\ Rank\\ Apdatation)\\ Tuning$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11430c3-38f5-4956-be66-017984e9645e",
   "metadata": {},
   "source": [
    "#### $3.1\\ (PEFT)\\ Parameter\\ Efficient\\ Fine\\ Tuning$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6e35db71-f07b-4f02-bc53-ac1971c67f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'] #Full Attention LoRA\n",
    ")\n",
    "\n",
    "model = get_peft_model(gemma3, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7367449c-9262-4998-bf3e-0328c2ad4ca9",
   "metadata": {},
   "source": [
    "`q_proj` = $Query\\ Projection$ \\\n",
    "`k_proj` = $Key\\ Projection$ \\\n",
    "`v_proj` = $Value\\ Projection$ \\\n",
    "`o_proj` = $Output\\ Projection$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e9421a8e-4e60-412f-b281-7ffb5523bcc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Gemma3ForCausalLM(\n",
      "      (model): Gemma3TextModel(\n",
      "        (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-17): 18 x Gemma3DecoderLayer(\n",
      "            (self_attn): Gemma3Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=640, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=640, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=640, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1024, out_features=640, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "            )\n",
      "            (mlp): Gemma3MLP(\n",
      "              (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
      "              (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
      "              (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
      "              (act_fn): PytorchGELUTanh()\n",
      "            )\n",
      "            (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "            (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "            (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "            (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "        (rotary_emb): Gemma3RotaryEmbedding()\n",
      "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940a27e-0103-49c6-9de8-5f6b106139e5",
   "metadata": {},
   "source": [
    "#### $3.2\\ Trainer$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37069bb-9cd0-416d-bfb3-43fc8234c558",
   "metadata": {},
   "source": [
    "##### $3.2.1\\ Create\\ Check-point$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7c01644b-1712-4373-a50e-074b425922d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5644510c-b6a2-4bdc-a3b8-45f4afb09818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs = 2,\n",
    "    learning_rate = 0.001,\n",
    "    logging_steps = 25,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    fp16 = False,\n",
    "    report_to = \"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c8c32d72-c529-4421-bff6-44b29274dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args = train_args,\n",
    "    model = model,\n",
    "    train_dataset = tokens['train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6c6d7033-e9b0-415c-8c13-eaac8c2fbbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kabil/anaconda3/envs/PythonCourse/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 01:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>7.926400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.497900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=72, training_loss=5.479748196072048, metrics={'train_runtime': 85.1949, 'train_samples_per_second': 3.31, 'train_steps_per_second': 0.845, 'total_flos': 21887884394496.0, 'train_loss': 5.479748196072048, 'epoch': 2.0})"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4455d5d-3108-4329-b116-db6d0e20bf02",
   "metadata": {},
   "source": [
    "##### $3.2.2 Restarter$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c57fecf2-aafc-44bb-8d91-39a05a26121f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training 2 epochs (from 1 to 2)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 : < :, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=72, training_loss=0.0, metrics={'train_runtime': 0.0067, 'train_samples_per_second': 42404.68, 'train_steps_per_second': 10826.727, 'total_flos': 21887884394496.0, 'train_loss': 0.0, 'epoch': 2.0})\n",
      "ðŸ˜´ Resting for 60 seconds to cool down...\n",
      "\n",
      "ðŸš€ Training 2 epochs (from 3 to 4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kabil/anaconda3/envs/PythonCourse/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [144/144 01:12, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.846400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=144, training_loss=1.5048340757687886, metrics={'train_runtime': 87.2219, 'train_samples_per_second': 6.466, 'train_steps_per_second': 1.651, 'total_flos': 43775768788992.0, 'train_loss': 1.5048340757687886, 'epoch': 4.0})\n",
      "ðŸ˜´ Resting for 60 seconds to cool down...\n",
      "\n",
      "ðŸš€ Training 2 epochs (from 5 to 6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kabil/anaconda3/envs/PythonCourse/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 01:10, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.278300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=216, training_loss=0.7891593156037507, metrics={'train_runtime': 75.7733, 'train_samples_per_second': 11.165, 'train_steps_per_second': 2.851, 'total_flos': 65663653183488.0, 'train_loss': 0.7891593156037507, 'epoch': 6.0})\n",
      "ðŸ˜´ Resting for 60 seconds to cool down...\n",
      "\n",
      "ðŸš€ Training 2 epochs (from 7 to 8)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kabil/anaconda3/envs/PythonCourse/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/288 01:09, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.889200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=288, training_loss=0.49246283372243244, metrics={'train_runtime': 71.3438, 'train_samples_per_second': 15.811, 'train_steps_per_second': 4.037, 'total_flos': 87551537577984.0, 'train_loss': 0.49246283372243244, 'epoch': 8.0})\n",
      "ðŸ˜´ Resting for 60 seconds to cool down...\n",
      "\n",
      "ðŸš€ Training 2 epochs (from 9 to 10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kabil/anaconda3/envs/PythonCourse/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [360/360 01:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.732200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.642300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=360, training_loss=0.33854348924424915, metrics={'train_runtime': 73.6246, 'train_samples_per_second': 19.151, 'train_steps_per_second': 4.89, 'total_flos': 109439421972480.0, 'train_loss': 0.33854348924424915, 'epoch': 10.0})\n",
      "ðŸ˜´ Resting for 60 seconds to cool down...\n",
      "\n",
      "ðŸš€ Training 2 epochs (from 11 to 12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kabil/anaconda3/envs/PythonCourse/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 01:09, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.803900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.555700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.480200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=432, training_loss=0.26310522468001757, metrics={'train_runtime': 71.7381, 'train_samples_per_second': 23.586, 'train_steps_per_second': 6.022, 'total_flos': 131327306366976.0, 'train_loss': 0.26310522468001757, 'epoch': 12.0})\n",
      "ðŸ˜´ Resting for 60 seconds to cool down...\n",
      "\n",
      "ðŸš€ Training 2 epochs (from 13 to 14)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kabil/anaconda3/envs/PythonCourse/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [504/504 01:12, Epoch 14/14]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.544500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.407100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.378500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=504, training_loss=0.20450282475304982, metrics={'train_runtime': 74.0481, 'train_samples_per_second': 26.658, 'train_steps_per_second': 6.806, 'total_flos': 153215190761472.0, 'train_loss': 0.20450282475304982, 'epoch': 14.0})\n",
      "ðŸ˜´ Resting for 60 seconds to cool down...\n",
      "\n",
      "ðŸš€ Training 2 epochs (from 15 to 16)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kabil/anaconda3/envs/PythonCourse/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/576 01:09, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.317400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=576, training_loss=0.17008561309840944, metrics={'train_runtime': 71.3161, 'train_samples_per_second': 31.634, 'train_steps_per_second': 8.077, 'total_flos': 175103075155968.0, 'train_loss': 0.17008561309840944, 'epoch': 16.0})\n",
      "ðŸ˜´ Resting for 60 seconds to cool down...\n",
      "\n",
      "ðŸš€ Training 2 epochs (from 17 to 18)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kabil/anaconda3/envs/PythonCourse/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='648' max='648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [648/648 01:10, Epoch 18/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.290500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=648, training_loss=0.14645184410942924, metrics={'train_runtime': 72.4416, 'train_samples_per_second': 35.035, 'train_steps_per_second': 8.945, 'total_flos': 196990959550464.0, 'train_loss': 0.14645184410942924, 'epoch': 18.0})\n",
      "ðŸ˜´ Resting for 60 seconds to cool down...\n",
      "\n",
      "ðŸš€ Training 2 epochs (from 19 to 20)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kabil/anaconda3/envs/PythonCourse/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [720/720 01:09, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.227300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=720, training_loss=0.12462990979353587, metrics={'train_runtime': 76.8088, 'train_samples_per_second': 36.715, 'train_steps_per_second': 9.374, 'total_flos': 218878843944960.0, 'train_loss': 0.12462990979353587, 'epoch': 20.0})\n",
      "ðŸ˜´ Resting for 60 seconds to cool down...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    #num_train_epochs = 10,\n",
    "    output_dir = \"./results\",\n",
    "    learning_rate = 0.001,\n",
    "    logging_steps = 25,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    fp16 = False,\n",
    "    report_to = \"none\"\n",
    ")\n",
    "\n",
    "total_epochs = 20\n",
    "chunk_size = 2\n",
    "rest_time = 60\n",
    "\n",
    "for start_epoch in range(0, total_epochs, chunk_size):\n",
    "    print(f\"\\nðŸš€ Training {chunk_size} epochs (from {start_epoch+1} to {start_epoch+chunk_size})...\")\n",
    "\n",
    "    train_args.num_train_epochs = start_epoch + chunk_size\n",
    "\n",
    "    trainer = Trainer(\n",
    "        args = train_args,\n",
    "        model = model,\n",
    "        train_dataset = tokens['train']\n",
    "    )\n",
    "    \n",
    "    train_out = trainer.train(resume_from_checkpoint=True)\n",
    "    print(train_out)\n",
    "\n",
    "    print(f\"ðŸ˜´ Resting for {rest_time} seconds to cool down...\")\n",
    "    time.sleep(rest_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb98986-4e9b-465e-bcdd-4851440a1051",
   "metadata": {},
   "source": [
    "## $$Perplexity\\ Calculation$$\n",
    "`Take: 1`\\\n",
    "$Training\\ Loss(TL) = 2.14$ \\\n",
    "$Epochs = 20$\n",
    "$$ Perplexity = e^{(TL)} = 8.75 $$\n",
    "\n",
    "`Take 2: (Full Attention)`\\\n",
    "$Training\\ Loss(TL) = 1.22$ \\\n",
    "$Epochs = 20$\n",
    "$$ Perplexity = e^{(TL)} = 3.38 $$\n",
    "\n",
    "Note:\n",
    "> Lower perplexity = better model (more confident and accurate).\\\n",
    "> `50` â†’ very poor (random-like) \\\n",
    "> `10â€“20` â†’ decent small model \\\n",
    "> `<5` â†’ pretty good (fluent) \\\n",
    "> `~1` â†’ near perfect "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb0336-27dc-4218-a65f-49b47ce3dd17",
   "metadata": {},
   "source": [
    "### <center>$4.\\ Saving\\ the$ `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "692fdddb-a2df-4cc4-b6f4-432c36a94231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Tuner/tokenizer_config.json',\n",
       " './Tuner/special_tokens_map.json',\n",
       " './Tuner/tokenizer.json')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model('./Tuner')\n",
    "trainer.state.save_to_json(\"./Tuner/trainer_state.json\")\n",
    "tokenizer.save_pretrained('./Tuner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d77637-724f-477f-9c34-bfb5906250d0",
   "metadata": {},
   "source": [
    "#### $4.1\\ Testing\\ the$ `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7cf70e15-3af8-455d-bb32-fcc2ba7ee0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What degrees does Kabil hold?\n",
      "Kabil graduated from the B.E.T.M.M. (Business and Management Extension) in 2013. This degree is designed to provide practical experience in business and management.<unused5256>Diharchester University Club is a non-profit organization that raises funds for scholarships and projects. suichenthttps://www.wikipedia.org is a non-profit organization dedicated to preserving and promoting the achievements of people. Another way for your comment to be considered is to be associated with a non-profit organization such aschesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchesterchester\n"
     ]
    }
   ],
   "source": [
    "lila = pipeline(\n",
    "    model = './Tuner',\n",
    "    tokenizer = './Tuner',\n",
    "    device = 'mps'\n",
    ")\n",
    "\n",
    "print(lila(\"What degrees does Kabil hold?\")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a785a2-8a60-4541-a2ef-c26a09cfe264",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826f4e71-6bd7-42c0-b040-a18af57780db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
